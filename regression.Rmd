---
title: "Prediction Analysis"
output: 
  html_document:
    toc: true
    toc_float: true
---

```{r,include=FALSE,message=FALSE,echo=FALSE}
library(tidyverse)
library(modelr)
library(mgcv)
library(prophet)
knitr::opts_chunk$set(
 echo = FALSE,
 fig.width = 7, 
 fig.height = 5,
 fig.asp = 0.6,
 out.width = "60%")
theme_set(theme_bw() + 
          theme(legend.position = "bottom",
                legend.title = element_blank(),
                plot.title = element_text(hjust = 0.5, size = 15),
                plot.subtitle = element_text(hjust = 0.5, size = 12)))

avo_df = 
  read_csv("data/avocado.csv") %>% 
  janitor::clean_names() %>% 
  select(-1) %>% 
  separate(date, c("year", "month", "day"), remove = FALSE) %>% 
  mutate(
    year = as.integer(year),
    month = as.integer(month),
    day = as.integer(day)
  ) %>% 
  rename(
    small = x4046,
    large = x4225,
    extra_large = x4770,
  ) 
```


## Use Prophet Machine Learning Model to Predict Price

Prophet is a great times-series prediction model. According to the graph below, we know where the average price will be at in the coming year. 

```{r, cache=TRUE, message=FALSE}
avo_df_sub = avo_df %>% 
  filter(region == "TotalUS") %>%
  select(date, average_price) %>%
  rename(ds = date, y = average_price) %>%
  mutate(ds = as.factor(ds))

mod <- prophet(avo_df_sub,  changepoint.prior.scale = 0.5)
future <- make_future_dataframe(mod, periods = 365)
# tail(future)
forecast <- predict(mod, future)
plot(mod, forecast, xlabel = "Date",
ylabel = "Average Price",
title = "Prediction of prices in one year")
```

## Forecast Components

The two graphs belows tells us the trend, yearly of the time series and how components of the model affect our predictions.
```{r, cache=TRUE, message=FALSE}
prophet_plot_components(mod, forecast)
```


```{r message = FALSE, warning = FALSE}
#avocado dataframe
avo_df = read_csv("data/avocado.csv") %>%
  janitor::clean_names() %>%
  select(-1) %>% 
  separate(date, c("year", "month", "day"), remove = FALSE) %>% 
  mutate(
    year = as.integer(year),
    month = as.integer(month),
    day = as.integer(day)
  ) %>% 
  rename(
    small = x4046,
    large = x4225,
    extra_large = x4770,
  ) 
```

# Regression1: price as an outcome with total volume and season as covariates
```{r}
reg1_df = 
  avo_df %>%
  filter(region == "TotalUS",
         type == "conventional") %>%
  mutate(season = if_else(month %in% c(3,4,5), "spring", if_else(month %in% c(6,7,8), "summer", if_else(month %in% c(9,10,11), "autum", "winter")))) %>%
  select(season, total_volume, average_price) %>%
  mutate(season = fct_infreq(season))

# plot
reg1_df %>%
  ggplot(aes(x = total_volume, y = average_price, color = season)) +
  geom_point()
```

```{r}
linear_mod1 = lm(average_price ~ total_volume+season, data = reg1_df)
# tidy output
linear_mod1 %>% 
  broom::tidy() %>% 
  select(term, estimate, p.value) %>% 
  mutate(term = str_replace(term, "^season", "Season: "))
```

Winter as reference. All predictor has p-value smaller than 0.05.

# Regression2: Average price vs total volume for type "conventional"
```{r}
# plot total volume (x) vs average price (y)
reg2_df = avo_df %>%
  filter(region == "TotalUS",
         type == "conventional")

reg2_df %>%
ggplot(aes(x = total_volume, y = average_price)) +
  geom_point()
```

# Cross-validation for average price vs total volume for type "conventional" for type "conventional"
```{r}
reg2_df = 
  reg2_df %>%
  mutate(volume_cp = (total_volume > 3.5*10^7) * (total_volume - 3.5*10^7))
```

```{r}
# conventional avocado, simple linear regression with total volume as x and average price as y
linear_mod2 = lm(average_price ~ total_volume, data = reg2_df)
pwl_mod = lm(average_price ~ total_volume+volume_cp, data = reg2_df)
smooth_mod = gam(average_price ~ s(total_volume), data = reg2_df)
```

```{r}
reg2_df %>% 
  gather_predictions(linear_mod2, pwl_mod, smooth_mod) %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = total_volume, y = average_price)) + 
  geom_point(alpha = .5) +
  geom_line(aes(y = pred), color = "red") + 
  facet_grid(~model)
```

```{r}
# cross-validation
cv_df =
  crossv_mc(reg2_df, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))

cv_df =
  cv_df %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))

cv_df = 
  cv_df %>% 
  mutate(
    linear_mod2  = map(train, ~lm(average_price ~ total_volume, data = .x)),
    pwl_mod     = map(train, ~lm(average_price ~ total_volume+volume_cp, data = .x)),
    smooth_mod  = map(train, ~gam(average_price ~ s(total_volume), data = as_tibble(.x)))) %>% 
  mutate(
    rmse_linear = map2_dbl(linear_mod2, test, ~rmse(model = .x, data = .y)),
    rmse_pwl    = map2_dbl(pwl_mod, test, ~rmse(model = .x, data = .y)),
    rmse_smooth = map2_dbl(smooth_mod, test, ~rmse(model = .x, data = .y)))
```

```{r}
# plot validation
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```

```{r}
# choose linear model
linear_mod2 %>% 
  broom::tidy() %>% 
  select(term, estimate, p.value)
```

```{r}
# use bootstrap to examine distribution of regression coefficients under repeated sampling.
set.seed(1)

reg2_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~ lm(average_price ~ total_volume, data = .x)),
    results = map(models, broom::tidy)) %>% 
  select(results) %>% 
  unnest(results) %>% 
  filter(term == "total_volume") %>% 
  ggplot(aes(x = estimate)) + geom_density()
```

```{r}
# bootstrap standard errors for the estimated regression coefficients
reg2_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(models = map(strap, ~lm(average_price ~ total_volume, data = .x) ),
         results = map(models, broom::tidy)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  group_by(term) %>% 
  summarize(boot_se = sd(estimate))
```

For a simple linear regression, we can show the fitted lines for each bootstrap sample to build intuition for these results.
```{r warning = FALSE}
# plot with bootstrap
boot_sample = function(df) {
  sample_frac(df, replace = TRUE)
}

boot_straps = 
  data_frame(
    strap_number = 1:1000,
    strap_sample = rerun(1000, boot_sample(reg2_df))
  )

boot_straps %>% 
  unnest(strap_sample) %>% 
  ggplot(aes(x = total_volume, y = average_price)) + 
  geom_line(aes(group = strap_number), stat = "smooth", method = "lm", se = FALSE, alpha = .1, color = "blue") +
  geom_point(data = reg2_df, alpha = .5)
```

# Hypothesis test: small model vs large model
```{r}
fit_null = lm(average_price ~ total_volume, data = reg2_df)
fit_alt = lm(average_price ~ total_volume + season, data = reg1_df)

anova(fit_null, fit_alt) %>% 
  broom::tidy()
```

Reject small model since p-value is smaller than 0.05. Regression 1 is a better fit regression discussing the association with average price as outcome with total volume and season as predictors.



